{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52d57ed-e7a4-44f5-9ad8-f67e282cf3e2",
   "metadata": {},
   "source": [
    "# Week 9 \n",
    "## In Class practice: files and the internet. \n",
    "\n",
    "Complete the tasks below, some will be in this notebook, others will need to be .py files. If you run .py files as part of this work, turn them in AND include screenshots in this notebook showing that you ran the code, results, how you modified it, etc. This Jupyter notebook should tell the story of the work that you have done, just like a lab notebook should.\n",
    "\n",
    "1. File format practice. Create a csv file (any way you would like, your choice), open it with Python, write some data to it, save it and close it. Re-open with python to see that your changes were made in the .csv file. Put this code in some cells below. When you turn in this notebook, also turn in the csv file that you have created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a333be2-0dee-4676-9a45-00f05a223563",
   "metadata": {},
   "source": [
    "2. Requests and BeautifulSoup. Below, write a few sentences describing what these are. Then, try them out on another website besides cnn.com. Modify the code below and display your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3617fb5-2b36-4965-91b2-c15329ae2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Name', 'Age', 'City']\n",
      "['John', '25', 'New York']\n",
      "['Alice', '30', 'Chicago']\n",
      "['Bob', '35', 'Los Angeles']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# if you need to install these you can use pip install <nameofmodule>, they are probably already in Colab\n",
    "\n",
    "#Requests is a Python library that simplifies making HTTP requests to web servers. It allows you to send GET, POST, and other types of requests to fetch web content.\n",
    "\n",
    "#BeautifulSoup is a library for parsing HTML and XML documents. It creates a parse tree from page source code that can be used to extract data. It makes it easy to search, navigate, and modify the parse tree, making it perfect for web scraping.\n",
    "\n",
    "import csv\n",
    "\n",
    "# Create and write to CSV\n",
    "with open('sample_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Name', 'Age', 'City'])\n",
    "    writer.writerow(['John', 25, 'New York'])\n",
    "    writer.writerow(['Alice', 30, 'Chicago'])\n",
    "    writer.writerow(['Bob', 35, 'Los Angeles'])\n",
    "\n",
    "# Read from CSV to verify\n",
    "with open('sample_data.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf5844e-480c-4117-bdf6-38f8a65948a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "#checking to see that beautifulSoup works\n",
    "html = \"<html><body><h1>Hello, World!</h1></body></html>\"\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "print(soup.h1.text)  # Output: Hello, World!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5142bf2-e1a4-4b68-a861-4213c88a3ac9",
   "metadata": {},
   "source": [
    "3. Webscraping with requests and BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2d62b27-45f9-4829-87f2-ffa839c566a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page title: Wikipedia\n"
     ]
    }
   ],
   "source": [
    "# Used a different website\n",
    "\n",
    "# Fetch Wikipedia's main page\n",
    "url = \"https://www.wikipedia.org\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "# Extract specific content\n",
    "title = soup.find(\"title\").text\n",
    "print(f\"Page title: {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c88f8a-e73b-4ed3-81a2-1c447f949ab3",
   "metadata": {},
   "source": [
    "3a. Try to extract another piece of data from a website- can you look through the source code in the html and see if there is another section of the website you chose for soup to find? I understand some of you have not learned html, but, you need to learn a bit of everything regardless! Print it to the screen and put your code and the result in a cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34364eb-19d8-4858-82e9-be9948753b3e",
   "metadata": {},
   "source": [
    "3b. Building Web APIs with Flask (somewhat advanced level)\n",
    "Description:\n",
    "Flask is a lightweight framework for building web applications and REST APIs. Read some documentation on it (and what is a REST API?)\n",
    "And make sure you can get the code below to run and display the message in your browser.\n",
    "Getting Started:\n",
    "Install Flask: pip install flask # if you are in VS code, if in Colab you might need a ! in front of the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68246ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all main page links\n",
    "links = soup.find_all('a', class_='central-featured-lang')\n",
    "for link in links[:5]:  # Show first 5 language options\n",
    "    print(f\"Language: {link.text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d3733",
   "metadata": {},
   "source": [
    "A REST (Representational State Transfer) API is an architectural style for designing networked applications. It uses HTTP requests to GET, PUT, POST, and DELETE data. RESTful APIs are stateless, meaning each request contains all necessary information, and they're designed to be simple and standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "604cbc00-82e5-49e8-bf34-2258c5fbd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622d2db1-5ee9-418d-85ad-b008fade1ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#Pay attention! The code below should be put in a file called app.py and run in VS Code, go to the browser where it is running\n",
    "#and what do you See?\n",
    "from flask import Flask, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Hello, Flask!\"\n",
    "\n",
    "@app.route('/api/data')\n",
    "def data():\n",
    "    return jsonify({\"message\": \"Hello, API!\"})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d51f04-1227-499e-a1cf-1ae3ec4cd97b",
   "metadata": {},
   "source": [
    "3c. Interacting with Web APIs using requests.\n",
    "\n",
    "The requests library makes it simple to send HTTP requests to APIs and handle responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99ea45-a706-46de-bb84-74afaa795ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch data from a public API\n",
    "url = \"https://api.coindesk.com/v1/bpi/currentprice.json\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse JSON response\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f\"Bitcoin Price (USD): {data['bpi']['USD']['rate']}\")\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12824944-c784-4ac9-ba9d-5536901de254",
   "metadata": {},
   "source": [
    "3d. Asynchronous Web Requests with aiohttp\n",
    "Description:\n",
    "aiohttp is an asynchronous library for making HTTP requests, allowing you to handle multiple requests concurrently.\n",
    "Look up aiohttp-- what is it used for?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4ada5-69e1-4367-a70c-40f0084e3dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8567d26",
   "metadata": {},
   "source": [
    "aiohttp is an asynchronous HTTP client/server framework for Python that uses Python's asyncio library. It's particularly useful when you need to make multiple HTTP requests concurrently, as it can handle them asynchronously (in parallel) rather than sequentially. This makes it much faster than traditional synchronous requests when dealing with multiple URLs or API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f8fcb-dca1-4b91-835b-489a6bd71e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warning- run this as a python program in VS code!\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch_url(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text()\n",
    "\n",
    "async def main():\n",
    "    urls = [\"https://cnn.com\", \"https://google.com\"]\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_url(session, url) for url in urls]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        for response in responses:\n",
    "            print(response[:100])  # Print first 100 characters of each response\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738a73b-9da2-4abc-90b4-4043c4b02d26",
   "metadata": {},
   "source": [
    "4. You should be at the point now with coding, that you are not just doing the bare minimum, but exploring what you would like to do with code. In the space below, jot down some ideas about what you might want to do with your lab. In the past, I've had fin math majors create a tracking spreadsheet python program. I've had others scrape data from the web for various uses, and then went on to build apps from this data in future classes. What you create for the lab can be simple, but starting to think about what you can do with programming, and then what you will need to learn to build robust solutions is the goal. So, a few ideas below AND please propose a cool idea in slack of what you can do with web libraries, flask, and files. (Web 1 students: yes! you can include this in your final web project if you want!) Turn this notebook, and any associated files, into Moodle (due Thursday end of class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c3033",
   "metadata": {},
   "source": [
    "Here are some potential project ideas combining web libraries, Flask, and files:\n",
    "\n",
    "1. Stock Market Dashboard\n",
    "   - Scrape real-time stock data from financial websites\n",
    "   - Store historical data in CSV files\n",
    "   - Create a Flask web app to display trends and analytics\n",
    "\n",
    "2. Weather Analysis Tool\n",
    "   - Fetch weather data from multiple cities using weather APIs\n",
    "   - Store historical weather data in CSV files\n",
    "   - Create visualizations and predictions using the collected data\n",
    "\n",
    "3. Social Media Analytics\n",
    "   - Use APIs to collect social media data\n",
    "   - Analyze sentiment and trends\n",
    "   - Create a Flask dashboard to display insights\n",
    "\n",
    "4. Personal Finance Tracker\n",
    "   - Create a Flask web app for expense tracking\n",
    "   - Store transactions in CSV files\n",
    "   - Add visualization of spending patterns\n",
    "   - Include cryptocurrency price tracking\n",
    "\n",
    "5. Web Crawler for News Aggregation\n",
    "   - Scrape news from multiple sources\n",
    "   - Categorize articles using NLP\n",
    "   - Create a Flask app to display personalized news feed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
